{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueberry Muffin vs Chihuahua - Building an Image Classifier\n",
    "## General Assembly Capstone Project\n",
    "\n",
    "\n",
    "\n",
    "### Problem Statement: \n",
    "\n",
    "**Background - Meme**: In 2016, a meme went viral that asked people a question they'd likely never thought would be challenging before: Can you tell the difference between these images that you never before thought looked alike? \n",
    "\n",
    "![title](./images/Other-Memes.png)\n",
    "(source: Elle Magazine, https://www.elle.com/culture/news/a34939/animals-or-food/)\n",
    "\n",
    "\n",
    "As the owner of a Chihuahua, my interest zeroed in on this pairing: \n",
    "![title](./images/Chihuahua.png)\n",
    "\n",
    "\n",
    "\n",
    "**Background - Image Classification**: A statement often made about image classification algorithms is that though they can quickly distinguish between thousands of images with *pretty good* accuracy, a child can distinguish between images with *much better accuracy*. \n",
    "\n",
    "The question of Chihuahua versus Blueberry Muffin fascinated me because--in the case of the particular close up angles selected for the meme--this is not a case when a human can easily distinguish between these images. \n",
    "\n",
    "\n",
    "\n",
    "**Problem Statement**: After building an image classification model that can predict whether an image is of a Chihuahua or a Blueberry Muffin trained on zoomed out distinctly different photos, can that image classification model accurately predict the classification for the challenging zoomed in photos from the meme? Additional questions I would like to explore include: \n",
    "- Based on the performance of the model, what can we determine about what the model is using to distinguish between the two classes? How does that differ from how a human distinguishes? Is it a better or worse system in the case of these memes?\n",
    "- Looking at the training data set of images, which images were missclassified with the highest likelihood of the other class? Do these images fit well with the images that went viral in the way that they are difficult to distinguish? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "I am still making modifications to fine tune my model to improve accuracy and reduce variance. I can provide results from my best model. \n",
    "\n",
    "Compared to a baseline accuracy of 50%, the model has a training accuracy of 85% and a validation accuracy of 78%. Given the challenge of the 16 photos in the meme, it predicted 12 out of 16 images correctly. Here is what it predicted: \n",
    "\n",
    "![](./images/meme_acc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Flow\n",
    "1. Scrape images from the internet\n",
    "2. Modify images to fill an array\n",
    "3. Build CNN to train and test on scraped images\n",
    "4. Run images from meme through CNN model\n",
    "5. Look for patterns in the data\n",
    "\n",
    "#### 1. Scrape Images from the Internet\n",
    "I explored a few free online libraries of image collections. It was not very hard to find an assortment of Chihuahua photos. However, there really was no great library of blueberry muffin photos. \n",
    "\n",
    "This led me to use a wrapper of Google Images' API. This was generally a great tool for polling images, but had one issue - Google Images' API only allows you to pull the 100 most recent images and the wrapper does not have a work around for this. \n",
    "\n",
    "In order to get around this issue, I started out by coming up with different descriptive words for blueberry muffin (jumbo, mini, Starbucks, Pete's, Vegan...). I quickly found that this led me to a lot of duplicates in the top 100 images that appear. \n",
    "\n",
    "The trick I found to find a unique new set of images was to translate \"blueberry muffin\" in to other languages and searching that term. There were issues with translations where there were accents on any letters (I needed to remove the accents) and I could not search words that were not in the Latin alphabet. I also found that in some cases, either the translation was wrong or blueberry muffins are made differently enough in that country that I didn't feel comfortable including those images. \n",
    "\n",
    "I have not yet been able to confirm from a native Vietnamese person or Finnish person if I have the correct translation for Blueberry Muffin or if these are the equivalent there, but the photos show the following: \n",
    "![title](./images/translations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Image Scraping Code</summary>\n",
    "<br>\n",
    "    This code is also included in a separate Jupyter Notebook\n",
    "    \n",
    "``` python\n",
    "# This code is modeled after stack overflow user Vicky Christina's code\n",
    "\n",
    "# Importing Google API wrapper\n",
    "from google_images_download import google_images_download \n",
    "import sys\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "# set up scraper\n",
    "f = open('URLS.txt', 'w')\n",
    "sys.stdout = f\n",
    "    \n",
    "# Image paths\n",
    "muffin_path = './muffin images/'\n",
    "chihuahua_path = './chihuahua/'\n",
    "\n",
    "# specify crop size of images to be used\n",
    "set_width = 200\n",
    "set_height = 200\n",
    "    \n",
    "# list of words to pull images for\n",
    "muffin_words = [\n",
    "    'blueberry muffin close-up', 'blueberries muffin', 'blueberries scone', \n",
    "    'blueberry muffin', 'blueberry muffin recipe', 'blueberry muffins', \n",
    "    'blueberry mufin', 'blueberry scone', 'bluebery muffin', 'bluebery mufin', \n",
    "    'one blueberry muffin', 'single blueberry muffin', 'Starbucks blueberry muffin', \n",
    "    \"Pete's blueberry muffin\", 'mini blueberry muffin', 'blueberry muffin top', \n",
    "    'giant blueberry muffin', 'low fat blueberry muffin', 'blueberry cupcake', \n",
    "    'jumbo blueberry muffin',  'blueberry muffin side view', 'blueberry muffin zoom', \n",
    "    'blueberry muffin bottom', 'blueberry minimuffin', 'blue berry muffin', \n",
    "    'blueberrymuffin', 'blue bery muffin', 'blueberymufin', 'blues muffin', \n",
    "    'berries muffin', 'blueberyy muffin', 'muffin de arandanos', 'muffin aux myrtilles', \n",
    "    'Blaubeermuffin', 'muffin fraochan', 'Bolinho de mirtilo', 'blabarsmuffin', \n",
    "    'bosbessenmuffin', 'muffin od borovnice', 'bloubessie muffin', 'borovnica za muffine', \n",
    "    'boruvkovy muffin', , 'blua mufino', 'mustika muffin', 'mustikkamuffinssi', \n",
    "    'blauwe muffin', 'muffin de arandanos', 'mellenu smalkmaizite', 'melyniu keksas', \n",
    "    'te kaeka mira', 'Muffin jagodowy', 'briosa cu afine', 'sulu silika', \n",
    "    'muffin subh-craoibhe', 'cucoriedkovy muffin', 'borovnicev muffin', 'buluug buluug ah', \n",
    "    'muffin buah beri biru', 'yabanmersinli kek', \"ko'k piyoz\", 'banh nÆ°ong xop viet quat', \n",
    "    'myffin llus', 'biriki muffin'\n",
    "    ]\n",
    "chihuaua_words = [\n",
    "    'cheagle', 'fat chihuahua', 'JackChis', 'ugly chihuahua', 'wet chihuahua', 'big chihuahua', \n",
    "    'chihuahua close-up', 'chihuahua ears', 'chihuahua face', 'chihuahua frown', \n",
    "    'chihuahua happy', 'chihuahua mouth', 'chihuahua nose', 'chihuahua puppy', 'chihuahua small', \n",
    "    'chihuahua smile', 'chihuahua tongue', 'chihuahua whiskers', 'chihuahua zoom', \n",
    "    'Chiwahwah', 'Chiwauwau', 'Chiwawa', 'Chiwawa puppy', 'chiweenie', 'chocolate brown chihuahua',\n",
    "    'light brown chihuahua', 'old chihuahua'\n",
    "    ]\n",
    "    \n",
    "for word in chihuaua_words: \n",
    "    response = google_images_download.googleimagesdownload()\n",
    "\n",
    "    arguments = {\"keywords\"     : word,\n",
    "                 \"limit\"        : 100,\n",
    "                 \"print_urls\"   : False,\n",
    "                 \"size\"         : \">2MP\",\n",
    "                 }\n",
    "    # saves each word's photo in to a folder named word under a folder named downloads\n",
    "    paths = response.download(arguments)\n",
    "\n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()\n",
    "    \n",
    "    # collecting and  URLs of images (I did not wind up using URLs)\n",
    "    with open('URLS.txt') as f:\n",
    "        content = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    urls = []\n",
    "    for j in range(len(content)):\n",
    "        if content[j][:9] == 'Completed':\n",
    "            urls.append(content[j-1][11:-1])  \n",
    "            \n",
    "for word in muffin_words: \n",
    "    response = google_images_download.googleimagesdownload()\n",
    "\n",
    "    arguments = {\"keywords\"     : word,\n",
    "                 \"limit\"        : 100,\n",
    "                 \"print_urls\"   : False,\n",
    "                 \"size\"         : \">2MP\",\n",
    "                 }\n",
    "    # saves each word's photo in to a folder named word under a folder named downloads\n",
    "    paths = response.download(arguments)\n",
    "\n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()\n",
    "    \n",
    "    # collecting and  URLs of images (I did not wind up using URLs)\n",
    "    with open('URLS.txt') as f:\n",
    "        content = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    urls = []\n",
    "    for j in range(len(content)):\n",
    "        if content[j][:9] == 'Completed':\n",
    "            urls.append(content[j-1][11:-1]) \n",
    "    \n",
    "    ```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to modifying the images, I manually moved the photos in to a folder of either Chihuahuas or Muffins, in the process deleting duplicates from searches. Next, I did a scan through the image set to catch any images that were not of the intended objects. I considered running an unsupervised model on the folder to catch the photos not of muffins or chihuahuas. However, I felt that I could more accurately sort the images manually and I was able to do so fairly quickly looking at the folder with large icons: \n",
    "![title](./images/window.png)\n",
    "\n",
    "Some frequent mistaken images I encountered were: \n",
    "![title](./images/mistakes.png)\n",
    "\n",
    "#### 2. Modify images to fill an array\n",
    "\n",
    "In order to take my image files and covert them to numeric values that could be interpretted by a neaural network, I took the following steps: \n",
    "- Create an empty array of \\[x, y, z\\] where x and y are my intended pixel dimensions for each images (300 x 300) and z is the number of images in the file set\n",
    "- Loop through every image to: \n",
    "    - Open the image with SkImage toolkit such that each opened image is an array of width of pixels x height of pixels x 3 colors (R, G, B). The value at each location represents the saturation of each color located each point.  \n",
    "    - Convert image to greyscale. This reduces the file size by saying only the saturation of black at each point\n",
    "    - Scale the image such that the smaller dimension is scaled up or down to 300 pixels\n",
    "    - Crop the larger dimension to be only 300 pixels (I opted to crop an equal amount from the top and bottom. \n",
    "    - Add image to empty array\n",
    "- Loop is run with try/except because some images that appeared viewable from Preview were unable to be processed by SkImage. These images were a small enough percent that I felt comfortable skipping past these to keep the loop running\n",
    "- Save array to my computer to be loaded in my next notebook\n",
    "![title](./images/processing1.png)\n",
    "![title](./images/processing2.png)\n",
    "\n",
    "Unfortunately, the dogs become much less cute by the time they're converted to a matrix of greyscale values. But it's the price we pay to effectively analyze our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Image Manipulation Code </summary>\n",
    "<br>\n",
    "    This code is also included in a separate Jupyter Notebook\n",
    "    \n",
    "```python\n",
    "# Importing libraries to access files; adjust images\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Image paths\n",
    "muffin_path = './muffin images/'\n",
    "chihuahua_path = './chihuahua/'\n",
    "\n",
    "# specify crop size of images to be used\n",
    "set_width = 200\n",
    "set_height = 200\n",
    "\n",
    " greyscale, crop, resize and add to an array a file of images \n",
    "\n",
    "image_list = os.listdir(muffin_path)\n",
    "\n",
    "# empty array to stack all images in\n",
    "m_array = np.zeros([set_width, set_height, len(image_list)])\n",
    "\n",
    "# loop through 100 images at a time\n",
    "for i, image_name in enumerate(image_list):\n",
    "    print(i)\n",
    "    try:\n",
    "        # reading image; making greyscale\n",
    "        image = rgb2gray(io.imread(muffin_path+image_name))\n",
    "\n",
    "        # loading height and width to determine: \n",
    "        # which dimension is smaller (to make 200)\n",
    "        # where to crop larger dimension to center image\n",
    "        image_height = image.shape[0]\n",
    "        image_width = image.shape[1]\n",
    "\n",
    "        if image_height > image_width:\n",
    "            #resizing\n",
    "            multiplier = set_width/image_width\n",
    "            new_height = int(image_height*multiplier)\n",
    "            image_resized = resize(image, (new_height, set_width), anti_aliasing=True)\n",
    "\n",
    "             #cropping\n",
    "            crop_cut = int((new_height-set_width)/2)\n",
    "            cropped = image_resized[crop_cut:crop_cut+set_width, 0:set_width]\n",
    "\n",
    "\n",
    "        else: \n",
    "            #resizing\n",
    "            multiplier = set_height/image_height\n",
    "            new_width = int(image_width*multiplier)\n",
    "            image_resized = resize(image, (set_width, new_width), anti_aliasing=True)\n",
    "\n",
    "            #cropping\n",
    "            crop_cut = int((new_width-set_width)/2)\n",
    "            cropped = image_resized[0:set_width, crop_cut:crop_cut+set_width]\n",
    "\n",
    "            # add to image stack\n",
    "\n",
    "        m_array[:,:,i] = cropped\n",
    "        if (i+1)%50 == 0:\n",
    "            np.save('./files/m_array.npy', m_array, True)\n",
    "            print(f'Processed {i+1} out of {len(image_list)} images.')\n",
    "\n",
    "    except:\n",
    "        print(f'Error at {image_name}')\n",
    "\n",
    "# greyscale, crop, resize and add to an array a file of images \n",
    "\n",
    "image_list = os.listdir(chihuahua_path)\n",
    "\n",
    "# empty array to stack all images in\n",
    "c_array = np.zeros([set_width, set_height, len(image_list)])\n",
    "\n",
    "# loop through 100 images at a time\n",
    "for i, image_name in enumerate(image_list):\n",
    "    print(i)\n",
    "    try:\n",
    "        # reading image; making greyscale\n",
    "        image = rgb2gray(io.imread(chihuahua_path+image_name))\n",
    "\n",
    "        # loading height and width to determine: \n",
    "        # which dimension is smaller (to make 200)\n",
    "        # where to crop larger dimension to center image\n",
    "        image_height = image.shape[0]\n",
    "        image_width = image.shape[1]\n",
    "\n",
    "        if image_height > image_width:\n",
    "            #resizing\n",
    "            multiplier = set_width/image_width\n",
    "            new_height = int(image_height*multiplier)\n",
    "            image_resized = resize(image, (new_height, set_width), anti_aliasing=True)\n",
    "\n",
    "             #cropping\n",
    "            crop_cut = int((new_height-set_width)/2)\n",
    "            cropped = image_resized[crop_cut:crop_cut+set_width, 0:set_width]\n",
    "\n",
    "\n",
    "        else: \n",
    "            #resizing\n",
    "            multiplier = set_height/image_height\n",
    "            new_width = int(image_width*multiplier)\n",
    "            image_resized = resize(image, (set_width, new_width), anti_aliasing=True)\n",
    "\n",
    "            #cropping\n",
    "            crop_cut = int((new_width-set_width)/2)\n",
    "            cropped = image_resized[0:set_width, crop_cut:crop_cut+set_width]\n",
    "\n",
    "            # add to image stack\n",
    "\n",
    "        c_array[:,:,i] = cropped\n",
    "        if (i+1)%50 == 0:\n",
    "            np.save('./files/c_array.npy', c_array, True)\n",
    "            print(f'Processed {i+1} out of {len(image_list)} images.')\n",
    "\n",
    "    except:\n",
    "        print(f'Error at {image_name}')\n",
    "\n",
    "# Creating files for 16 images from meme\n",
    "# creating file of 16 meme test images\n",
    "\n",
    "image_list1 = os.listdir('./test images/chihuahua/')\n",
    "image_list2 = os.listdir('./test images/muffin/')\n",
    "\n",
    "# empty array to stack all images in\n",
    "test_array = np.zeros([set_width, set_height, len(image_list1)+len(image_list2)])\n",
    "\n",
    "\n",
    "for i, image_name in enumerate(image_list1):\n",
    "    print(i)\n",
    "    try:\n",
    "        # reading image; making greyscale\n",
    "        image = rgb2gray(io.imread('./test images/chihuahua/'+image_name))\n",
    "\n",
    "        # loading height and width to determine: \n",
    "        # which dimension is smaller (to make 200)\n",
    "        # where to crop larger dimension to center image\n",
    "        image_height = image.shape[0]\n",
    "        image_width = image.shape[1]\n",
    "\n",
    "        if image_height > image_width:\n",
    "            #resizing\n",
    "            multiplier = set_width/image_width\n",
    "            new_height = int(image_height*multiplier)\n",
    "            image_resized = resize(image, (new_height, set_width), anti_aliasing=True)\n",
    "\n",
    "             #cropping\n",
    "            crop_cut = int((new_height-set_width)/2)\n",
    "            cropped = image_resized[crop_cut:crop_cut+set_width, 0:set_width]\n",
    "\n",
    "\n",
    "        else: \n",
    "            #resizing\n",
    "            multiplier = set_height/image_height\n",
    "            new_width = int(image_width*multiplier)\n",
    "            image_resized = resize(image, (set_width, new_width), anti_aliasing=True)\n",
    "\n",
    "            #cropping\n",
    "            crop_cut = int((new_width-set_width)/2)\n",
    "            cropped = image_resized[0:set_width, crop_cut:crop_cut+set_width]\n",
    "\n",
    "            # add to image stack\n",
    "\n",
    "        test_array[:,:,i] = cropped\n",
    "        \n",
    "    except:\n",
    "        print(f'Error at {image_name}')\n",
    "        \n",
    "for i, image_name in enumerate(image_list2):\n",
    "    j=i+8\n",
    "    print(j)\n",
    "    try:\n",
    "        # reading image; making greyscale\n",
    "        image = rgb2gray(io.imread('./test images/muffin/'+image_name))\n",
    "\n",
    "        # loading height and width to determine: \n",
    "        # which dimension is smaller (to make 200)\n",
    "        # where to crop larger dimension to center image\n",
    "        image_height = image.shape[0]\n",
    "        image_width = image.shape[1]\n",
    "\n",
    "        if image_height > image_width:\n",
    "            #resizing\n",
    "            multiplier = set_width/image_width\n",
    "            new_height = int(image_height*multiplier)\n",
    "            image_resized = resize(image, (new_height, set_width), anti_aliasing=True)\n",
    "\n",
    "             #cropping\n",
    "            crop_cut = int((new_height-set_width)/2)\n",
    "            cropped = image_resized[crop_cut:crop_cut+set_width, 0:set_width]\n",
    "\n",
    "\n",
    "        else: \n",
    "            #resizing\n",
    "            multiplier = set_height/image_height\n",
    "            new_width = int(image_width*multiplier)\n",
    "            image_resized = resize(image, (set_width, new_width), anti_aliasing=True)\n",
    "\n",
    "            #cropping\n",
    "            crop_cut = int((new_width-set_width)/2)\n",
    "            cropped = image_resized[0:set_width, crop_cut:crop_cut+set_width]\n",
    "\n",
    "            # add to image stack\n",
    "\n",
    "        test_array[:,:,j] = cropped\n",
    "    \n",
    "    except:\n",
    "        print(f'Error at {image_name}')\n",
    "\n",
    "np.save('./files/test_array.npy', test_array, True)\n",
    "    \n",
    "    ```  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build CNN to train and test on scraped images\n",
    "\n",
    "I created several versions of models using Keras Convolutional Neural Networks. My goal was to optimize the following metrics. Different metrics were prioritized at different stages of my model building: \n",
    "- **Run Time** - Early on, I wanted the models to run quickly to enable several iterations to get a rough idea of what will perform well at a very high level. \n",
    "- **Accuracy** - This is the most obvious thing to optimize. This is a case where there is no reason to optimize for sensitivity or specificity, so accuracy is a great metric to check how well my model runs.\n",
    "- **Reducing Variance** - I found that the models with the highest train accuracy score improved train accuracy to the detriment of my validation accuracy. I am targeting models which have about the same accuracies for my training set and my validation set. \n",
    "- **Test Accuracy** - My capstone problem statement asks if an image classifier can distinguish between the 16 images from the *Blueberry Muffin or Chihuahua Meme*. After I have created a model with high accuracy, I want to check if it can accomplish this task. Whether the model can or can not, I want to extract information from the images the program either did or did not classify correctly. \n",
    "\n",
    "The parameters I modified as I iterated through models included: \n",
    "- **Images**\n",
    "    - **Image Sizes** - Square images of edge lengths 300 pixels, 200 pixels, 100 pixels\n",
    "    - **Color** - Greyscale images (each image is a numpy array of length, width, 1) or Color images (each image is a numpy array of length, width, 3)\n",
    "- **Convolutional Layers**\n",
    "    - **Number of Filters** \n",
    "    - **Kernel Size**\n",
    "    - **Pooling Size**\n",
    "- **Dense Layers**\n",
    "    - **Units**\n",
    "- **All Layers**\n",
    "    - **Number of Layerss**\n",
    "    - **Dropout Layers and Dropout Percents**\n",
    "- **Model Fitting**\n",
    "    - **Batch Size**\n",
    "    - **Epochs**\n",
    "    - **Callbacks/Learning Rate**\n",
    "\n",
    "I was able to get a few models to a validation accuracy of about 80%. Please refer to the github to view all iterations of the model I ran through AWS. I have pasted the model I selected as my \"best\" below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Image Classification Code </summary>\n",
    "<br>\n",
    "   \n",
    "```python\n",
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import ReduceLROnPlateau, Callback\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "# Loading dataset\n",
    "chihuahua_file = np.load('./files/c_array.npy')\n",
    "muffin_file = np.load('./files/m_array.npy')\n",
    "\n",
    "# Loading test set\n",
    "test_array = np.load('./files/test_array.npy')\n",
    "test_array = np.transpose(test_array, (3, 0, 1, 2))\n",
    "\n",
    "\n",
    "files = np.append(chihuahua_file, muffin_file, axis = 2)\n",
    "\n",
    "# Creating file of output values, \n",
    "# muffin is positive class chihuahua is negative class\n",
    "y = np.append(np.ones(chihuahua_file.shape[2]),\n",
    "              np.zeros(muffin_file.shape[2]))\n",
    "\n",
    "# Reshaping y such that it can be input in to neural net \n",
    "# for greyscale analysis\n",
    "X = np.transpose(files, (3, 0, 1, 2))\n",
    "\n",
    "# Train, test, split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Setting up callback and learning\n",
    "class myCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('acc')>.90):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True            \n",
    "callbacks=myCallback()\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.3, verbose=1,\n",
    "                              patience=2, min_lr=0.00000001)\n",
    "\n",
    "# Creating a convolutional neural network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(\n",
    "    filters = 45,\n",
    "    kernel_size = (10,10),\n",
    "    activation = 'relu',\n",
    "    input_shape = (150, 150, 1)\n",
    "))\n",
    "model.add(MaxPooling2D(pool_size = (2)))\n",
    "\n",
    "model.add(Conv2D(32,\n",
    "                     kernel_size=6,\n",
    "                     activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=40,\n",
    "                 verbose=1,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 callbacks=[reduce_lr, callbacks]\n",
    "                 )\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# graphing training and testing loss scores\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
    "plt.plot(test_loss, label='Testing Loss', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "\n",
    "plt.legend(fontsize = 18);\n",
    "plt.show()\n",
    "\n",
    "model.save('./files/model_aws13.HDF5')\n",
    "\n",
    "print(model.predict(test_array))\n",
    "\n",
    "\n",
    "# graphing training and testing loss scores\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
    "plt.plot(test_loss, label='Validation Loss', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Validation Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "\n",
    "plt.legend(fontsize = 18)\n",
    "plt.savefig('./files/loss13.png')\n",
    "plt.show();\n",
    "\n",
    "train_acc = history.history['acc']\n",
    "test_acc = history.history['val_acc']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_acc, label='Training Accuracy', color='#185fad')\n",
    "plt.plot(test_acc, label='Validation Accuracy', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "\n",
    "plt.legend(fontsize = 18)\n",
    "plt.savefig('./files/acc13.png')\n",
    "plt.show();\n",
    "\n",
    "np.save('./files/train_acc13.png', train_acc)\n",
    "np.save('./files/test_acc13.png', test_acc)\n",
    "np.save('./files/train_loss13.png', train_loss)\n",
    "np.save('./files/test_loss13.png', test_loss)\n",
    "    ```  \n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
